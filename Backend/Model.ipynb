{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\karan\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import numpy as np\n",
    "import missingno\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, fbeta_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras import optimizers, Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_data.csv\")\n",
    "timestep = 8 #from 1 to 23 (17 with the current NaN strategy)\n",
    "threshold_for_classification = -6.5\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "fill_X = -0.01\n",
    "seed = 11\n",
    "features = ['event_id','time_to_tca', 'risk', 'c_time_lastob_end', \n",
    "             'c_time_lastob_start', 'max_risk_estimate',\n",
    "             'max_risk_scaling']\n",
    "\n",
    "#RNN hiperparameter\n",
    "epochs = 100\n",
    "batch = 256\n",
    "val_split = 0.25\n",
    "test_split = 0.25\n",
    "lr = 0.0001\n",
    "adam = optimizers.Adam() #(lr)\n",
    "class_weight = {True:  300.,\n",
    "                False: 1.}\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"c_rcs_estimate\", axis=1)\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "#Filtering events with len=1 or min_tca > 2 or max_tca < 2\n",
    "def conditions(event):\n",
    "    x = event[\"time_to_tca\"].values\n",
    "    return ((x.min()<2.0) & (x.max()>2.0) & (x.shape[0]>1))\n",
    "df = df.groupby('event_id').filter(conditions)\n",
    "\n",
    "#OHE for c_object_type (5 categories) -> 5 new features\n",
    "df[\"mission_id\"] = df[\"mission_id\"].astype('category')\n",
    "df[\"c_object_type\"] = df[\"c_object_type\"].astype('category')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "#Getting y as 1D-array\n",
    "y = df.groupby([\"event_id\"])[\"risk\"].apply(lambda x: x.iloc[-1]).values.reshape(-1, 1)\n",
    "\n",
    "#Scaling y\n",
    "_ = y_scaler.fit(df[\"risk\"].values.reshape(-1, 1)) #using the whole risk feature to scale the target 'y'\n",
    "y = y_scaler.transform(y)\n",
    "\n",
    "#Getting X as df (dropping rows with tca < 2) \n",
    "df = df.loc[df[\"time_to_tca\"]>2]\n",
    "\n",
    "#Scaling X\n",
    "df = pd.DataFrame(X_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df = df[features]\n",
    "\n",
    "#Transforming X into a 3D-array\n",
    "events = df[\"event_id\"].nunique() #rows\n",
    "features = len(df.columns) #columns\n",
    "\n",
    "X = np.zeros((events,timestep,features))\n",
    "X.fill(fill_X)\n",
    "\n",
    "i = 0\n",
    "def df_to_3darray(event):\n",
    "    global X, i\n",
    "    #Transforming an event to time series (1,timesteps, columns)\n",
    "    row = event.values.reshape(1,event.shape[0],event.shape[1])\n",
    "    #Condition is needed to slice arrays correctly\n",
    "    #Condition -> is timestep greater than the event's time series length? \n",
    "    if(timestep>=row.shape[1]):\n",
    "        X[i:i+1,-row.shape[1]:,:] = row\n",
    "    else:\n",
    "        X[i:i+1,:,:] = row[:,-timestep:,:]\n",
    "    #index to iterate over X array\n",
    "    i = i + 1\n",
    "    #dataframe remains intact, while X array has been filled.\n",
    "    return event\n",
    "\n",
    "df.groupby(\"event_id\").apply(df_to_3darray)\n",
    "\n",
    "#Dropping event_id to remove noise\n",
    "X = X[:,:,1:]\n",
    "\n",
    "#TODO: Padding with specific values column-wise instead of zeros.\n",
    "#TODO: Separating time dependent and independent feature in 2 X arrays\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing scaled threshold \n",
    "th = np.array([threshold_for_classification]).reshape(-1,1)\n",
    "th = y_scaler.transform(th)\n",
    "threshold_scaled = th[0,0]\n",
    "\n",
    "#Splitting arrays\n",
    "y_boolean = (y > threshold_scaled).reshape(-1,1)\n",
    "X_train, X_test, y_train_numeric, y_test_numeric = train_test_split(X, y, \n",
    "                                                    stratify=y_boolean, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=seed,\n",
    "                                                    test_size = test_split\n",
    "                                                  )\n",
    "\n",
    "y_train_boolean = (y_train_numeric > threshold_scaled).reshape(-1,1)\n",
    "X_train, X_val, y_train_numeric, y_val_numeric = train_test_split(X_train, y_train_numeric, \n",
    "                                                    stratify=y_train_boolean, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=seed,\n",
    "                                                    test_size = val_split\n",
    "                                                  )\n",
    "#transforming it into a classification task -> y_train, y_test boolean\n",
    "y_train = (y_train_numeric > threshold_scaled).reshape(-1,1)\n",
    "y_val = (y_val_numeric > threshold_scaled).reshape(-1,1)\n",
    "y_test = (y_test_numeric > threshold_scaled).reshape(-1,1)\n",
    "\n",
    "#Percentage of high risks in train\n",
    "print(\"TRAIN {:0.1f}, {:0.1f}, {:0.3f}\".format(np.sum(y_train), y_train.shape[0], np.sum(y_train)/y_train.shape[0]))\n",
    "#Percentage of high risks in val\n",
    "print(\"VAL   {:0.1f}, {:0.1f}, {:0.3f}\".format(np.sum(y_val), y_val.shape[0], np.sum(y_val)/y_val.shape[0]))\n",
    "#Percentage of high risks in test\n",
    "print(\"TEST  {:0.1f}, {:0.1f}, {:0.3f}\".format(np.sum(y_test), y_test.shape[0], np.sum(y_test)/y_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
